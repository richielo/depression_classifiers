{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatten to a single list of post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>untokenized_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1376262679</td>\n",
       "      <td>I can't fucking read the signs of the protestors, maybe it's just the res on my phone app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1376278407</td>\n",
       "      <td>No people say km for kilometres not k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1376374810</td>\n",
       "      <td>Yesssss KOTOR just bought both for like $2 on steam during the sale and replaying :D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1376378867</td>\n",
       "      <td>Oh my fuck the cats face in the corners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1376438053</td>\n",
       "      <td>Do you spend much time on places like Reddit? How does it feel seeing Bryan and yourself and other Breaking Bad references come up so often, bitch?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1376448214</td>\n",
       "      <td>That's the point....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1376728724</td>\n",
       "      <td>Good karma...hehe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1376729322</td>\n",
       "      <td>Just bought for $2 on steam thanks to the summer sale and am currently replaying :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1376804428</td>\n",
       "      <td>5 food groups? It's fruits and vegetables together....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1376822284</td>\n",
       "      <td>You're what's fucking wrong with the world</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  \\\n",
       "0  1376262679   \n",
       "1  1376278407   \n",
       "2  1376374810   \n",
       "3  1376378867   \n",
       "4  1376438053   \n",
       "5  1376448214   \n",
       "6  1376728724   \n",
       "7  1376729322   \n",
       "8  1376804428   \n",
       "9  1376822284   \n",
       "\n",
       "                                                                                                                                      untokenized_post  \n",
       "0  I can't fucking read the signs of the protestors, maybe it's just the res on my phone app...                                                         \n",
       "1  No people say km for kilometres not k                                                                                                                \n",
       "2  Yesssss KOTOR just bought both for like $2 on steam during the sale and replaying :D                                                                 \n",
       "3  Oh my fuck the cats face in the corners                                                                                                              \n",
       "4  Do you spend much time on places like Reddit? How does it feel seeing Bryan and yourself and other Breaking Bad references come up so often, bitch?  \n",
       "5  That's the point....                                                                                                                                 \n",
       "6  Good karma...hehe                                                                                                                                    \n",
       "7  Just bought for $2 on steam thanks to the summer sale and am currently replaying :)                                                                  \n",
       "8  5 food groups? It's fruits and vegetables together....                                                                                               \n",
       "9  You're what's fucking wrong with the world                                                                                                           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "def flatten(z):\n",
    "   return [x for y in z for x in y]\n",
    "\n",
    "# with open('data/xaa') as f:\n",
    "#     for line in f:\n",
    "#         data = json.load(line)\n",
    "#         print(data)\n",
    "#     posts_list = map(lambda x: x['posts'], data)\n",
    "#     posts_list_f = flatten(posts_list)\n",
    "#     pprint(flatten(posts_list))\n",
    "#     message_list = map(lambda x: x[1], flatten(posts_list))\n",
    "#     train = pd.DataFrame(posts_list_f, columns=['timestamp','untokenized_post'])\n",
    "#     display(message_list)\n",
    "\n",
    "contents = open('data/xaa', \"r\").read() \n",
    "data = [json.loads(str(item)) for item in contents.strip().split('\\n')]\n",
    "user_list = map(lambda x: x[0]['posts'], data)\n",
    "post_list = flatten(user_list)\n",
    "posts_df = pd.DataFrame(post_list, columns=['timestamp','untokenized_post'])\n",
    "\n",
    "display(posts_df.head(10))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer, WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "twitter_tknzr = TweetTokenizer()\n",
    "wp_tknzr = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if b is a substring of a\n",
    "def contains(a, b):\n",
    "    if b in a:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "# Check if elem exists in a lst\n",
    "def exists(lst, elem):\n",
    "    return next((1 for x in lst if contains(x,elem)), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _words = single words, check with exact match against a word\n",
    "# _phrases = words or phrases, checked with substring against the whole sentence\n",
    "\n",
    "# Read in NRC Lexicon\n",
    "nrc_emo_words = pd.read_csv('sentiment_lexicons/en/single_word/nrc-emotion-words-v0.92.txt', sep=\"\\t\", names=[\"word\",\"emotion\",\"isEmotion\"])\n",
    "nrc_pos_words = nrc_emo_words[(nrc_emo_words.emotion == 'positive') & (nrc_emo_words.isEmotion == 1)]\n",
    "nrc_pos_words_list = list(nrc_pos_words.word)\n",
    "nrc_neg_words = nrc_emo_words[(nrc_emo_words.emotion == 'negative') & (nrc_emo_words.isEmotion == 1)]\n",
    "nrc_neg_words_list = list(nrc_neg_words.word)\n",
    "\n",
    "# Read in BingLiu Lexicon\n",
    "bingliu_pos_words = pd.read_csv('sentiment_lexicons/en/single_word/bingliu-positive-words.txt', names=[\"word\"])\n",
    "bingliu_pos_words_list = list(bingliu_pos_words.word)\n",
    "bingliu_neg_words = pd.read_csv('sentiment_lexicons/en/single_word/bingliu-negative-words.txt', names=[\"word\"])\n",
    "bingliu_neg_words_list = list(bingliu_neg_words.word)\n",
    "\n",
    "# Read in HowNet Lexicon\n",
    "hn_subj_phrases = pd.read_csv('sentiment_lexicons/en/phrase/hn-subjective-words-modified.txt', names=[\"word\"], skiprows=1)\n",
    "hn_subj_phrases_list = list(hn_subj_phrases.word)\n",
    "\n",
    "hn_neg_phrases = pd.read_csv('sentiment_lexicons/en/phrase/hn-negative-words.txt', names=[\"word\"], skiprows=1)\n",
    "hn_neg_phrases_list = list(hn_neg_phrases.word)\n",
    "\n",
    "hn_deg_adv_phrases = pd.read_csv('sentiment_lexicons/en/phrase/hn-degree-adverb-words-modified.txt', names=[\"word\"], skiprows=1)\n",
    "hn_deg_adv_phrases_list = list(hn_deg_adv_phrases.word)\n",
    "\n",
    "# Read in MPQA Subjectivity Lexicon\n",
    "fs = open('sentiment_lexicons/en/phrase/mpqa-subjectivity-lexicon.tff').read()\n",
    "lines = [x for x in fs.strip().split('\\n')]\n",
    "def parseKeyValuePair(line):\n",
    "    return dict([kv_pair.split('=') for kv_pair in line.split(' ')])\n",
    "mpqa_subj_words = pd.DataFrame(list(map(parseKeyValuePair, lines)))\n",
    "mpqa_weak_subj_words = mpqa_subj_words[(mpqa_subj_words.type == 'weaksubj')]\n",
    "# NOTE: Take strong subjectivity words as \"Subjective\"\n",
    "mpqa_strong_subj_words = mpqa_subj_words[(mpqa_subj_words.type == 'strongsubj')]\n",
    "mpqa_strong_subj_words_list = list(mpqa_strong_subj_words.word1)\n",
    "\n",
    "# mpqa_subj_phrases = pd.read_csv('sentiment_lexicons/en/phrase/mpqa-subjectivity-lexicon.tff')\n",
    "# display(mpqa_subj_phrases)\n",
    "\n",
    "# Compile list of words\n",
    "# Positive sentiment\n",
    "pos_sent_words_list = nrc_pos_words_list + bingliu_pos_words_list\n",
    "# Negative sentiment\n",
    "neg_sent_words_list = nrc_neg_words_list + bingliu_neg_words_list\n",
    "# Subjective phrases\n",
    "subj_phrases_list = mpqa_strong_subj_words_list\n",
    "# Negative word\n",
    "neg_phrases_list = hn_neg_phrases_list\n",
    "# Degree adverb word\n",
    "deg_adv_phrases_list = hn_deg_adv_phrases_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 13048 records\n",
      "423.2522923719953\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "import concurrent.futures\n",
    "def getLexiconVector(text):\n",
    "    tokens = wp_tknzr.tokenize(text)\n",
    "    # tag POS\n",
    "    tokens_tagged = nltk.pos_tag(tokens)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # remove stopwords, filter punctuation\n",
    "    words_tagged = [(word_tagged[0].lower(), word_tagged[1]) for word_tagged in tokens_tagged if (not word_tagged[0] in stop_words) and word_tagged[0].isalpha() ]\n",
    "    lex_vecs = []\n",
    "    for word_tagged in words_tagged:\n",
    "        lex_vec = np.zeros([1,10])\n",
    "        # positive\n",
    "        lex_vec[0][0] = exists(pos_sent_words_list, word_tagged[0])\n",
    "        # negative\n",
    "        lex_vec[0][1] = exists(neg_sent_words_list, word_tagged[0])\n",
    "        # neutral\n",
    "        lex_vec[0][2] = not(lex_vec[0][0]) and not(lex_vec[0][1])\n",
    "        # objective\n",
    "        lex_vec[0][3] = not(exists(subj_phrases_list, word_tagged[0]))\n",
    "        # negative\n",
    "        lex_vec[0][4] = exists(neg_phrases_list, word_tagged[0])\n",
    "        # degree adverb\n",
    "        lex_vec[0][5] = exists(deg_adv_phrases_list, word_tagged[0])\n",
    "        # noun\n",
    "        lex_vec[0][6] = word_tagged[1].startswith('NN')\n",
    "        # verb\n",
    "        lex_vec[0][7] = word_tagged[1].startswith('VB')\n",
    "        # adjective\n",
    "        lex_vec[0][8] = word_tagged[1].startswith('JJ')\n",
    "        # adverb\n",
    "        lex_vec[0][9] = word_tagged[1].startswith('RB')\n",
    "        lex_vecs.append(lex_vec)\n",
    "    if len(lex_vecs) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        return np.stack(lex_vecs).reshape(len(lex_vecs), 10, 1)\n",
    "\n",
    "# sentence = \"Good karma...hehe\"\n",
    "# getLexiconVector(sentence)\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    count = 0\n",
    "    records = list(posts_df.untokenized_post\n",
    "                  )\n",
    "    feat_matrix_list = list(executor.map(getLexiconVector, records))\n",
    "    print(\"Processed\", len(feat_matrix_list), \"records\")\n",
    "    duration = timer() - start\n",
    "    print(duration)\n",
    "            \n",
    "# lex_vec_list = [getLexiconVector(post) for post in list(posts_df.untokenized_post.head(1000))]\n",
    "# for lex_vec in lex_vec_list:\n",
    "#     print(lex_vec.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-96c6ad188836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# adjective\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# adverb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# 1 x 10 matrix\n",
    "# +ve sentiment\n",
    "# -ve sentiment\n",
    "# neutral\n",
    "# objective\n",
    "# negative\n",
    "# degree adverb\n",
    "# noun\n",
    "# verb\n",
    "# adjective\n",
    "# adverb\n",
    "a = np.zeros(shape=(10,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

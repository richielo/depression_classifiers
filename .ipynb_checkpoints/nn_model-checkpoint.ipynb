{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import statistics\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import text_features_extractor as tfExtractor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, accuracy_score, recall_score\n",
    "#Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import plot_model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import tensorflow\n",
    "from lime import lime_text\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN model for Chinese data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the csv\n",
    "chinDataDf = pd.read_csv(\"data_with_features/chin_cleaned_data_f.csv\", encoding='UTF-8')\n",
    "#Only take the text and sentiment columns\n",
    "chinDataDf = chinDataDf[['text', 'depressed']]\n",
    "#Cleaning\n",
    "for index, row in chinDataDf.iterrows():\n",
    "    #Preprocessing\n",
    "    chinText, engText = tfExtractor.splitChinEng(row['text'])\n",
    "    text = tfExtractor.chinPreprocessing(chinText)\n",
    "    chinDataDf.set_value(index,'text',text)\n",
    "#Convert data to numpy array\n",
    "X = np.array(chinDataDf['text'].tolist())\n",
    "Y = np.array(chinDataDf['depressed'].tolist())\n",
    "#Convert -1 label to 0\n",
    "i = 0\n",
    "for label in Y:\n",
    "    if(label == -1):\n",
    "        Y[i] = 0\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer fitting is complete\n",
      "Number of words: 31152\n"
     ]
    }
   ],
   "source": [
    "#Original number of words: 46708\n",
    "#Set top words\n",
    "topWords = 5000\n",
    "#Tokenizing the data\n",
    "tokenizer = Tokenizer(num_words=topWords)\n",
    "xString = []\n",
    "for text in X:\n",
    "    xString.append(' '.join(text))\n",
    "tokenizer.fit_on_texts(xString)\n",
    "print(\"tokenizer fitting is complete\")\n",
    "xSeq = tokenizer.texts_to_sequences(xString)\n",
    "wordIndex = tokenizer.word_index\n",
    "print(\"Number of words: \" + str(len(wordIndex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save tokenizer \n",
    "pickle.dump(tokenizer, open('saved_model/chinTokenizer.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review mean length: 273.425373134\n",
      "maximum review length: 583\n",
      "Done padding\n"
     ]
    }
   ],
   "source": [
    "#Get review mean length\n",
    "lengths = [len(i) for i in xSeq]\n",
    "print(\"review mean length: \" + str(np.mean(lengths)))\n",
    "#Set maximum review length to cover at least 90% of review content\n",
    "maxReviewLength = int(np.percentile(lengths, 90))\n",
    "print(\"maximum review length: \" + str(maxReviewLength))\n",
    "\n",
    "#Set paddings for review data\n",
    "xPadded = pad_sequences(xSeq, maxlen=maxReviewLength)\n",
    "print(\"Done padding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def creatNnModel():\n",
    "    # Simple multilayer perceptron model\n",
    "    model = Sequential()\n",
    "    #Embedding layer\n",
    "    model.add(Embedding(topWords, 128, input_length=maxReviewLength))\n",
    "    #Flattening\n",
    "    model.add(Flatten())\n",
    "    #Hidden layer\n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    #Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    return model\n",
    "\n",
    "def createCnnModel():\n",
    "    #CNN model\n",
    "    cnnModel = Sequential()\n",
    "    cnnModel.add(Embedding(topWords, 128, input_length=maxReviewLength))\n",
    "    cnnModel.add(Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
    "    cnnModel.add(MaxPooling1D(pool_size=2))\n",
    "    cnnModel.add(Flatten())\n",
    "    cnnModel.add(Dense(250, activation='relu'))\n",
    "    cnnModel.add(Dense(1, activation='sigmoid'))\n",
    "    cnnModel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #print(cnnModel.summary())\n",
    "    return cnnModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training session: 0\n",
      "Train on 643 samples, validate on 161 samples\n",
      "Epoch 1/25\n",
      "4s - loss: 0.9119 - acc: 0.5630 - val_loss: 0.6170 - val_acc: 0.6708\n",
      "Epoch 2/25\n",
      "3s - loss: 0.4814 - acc: 0.7823 - val_loss: 0.6071 - val_acc: 0.6584\n",
      "Epoch 3/25\n",
      "3s - loss: 0.3436 - acc: 0.8725 - val_loss: 0.5566 - val_acc: 0.7143\n",
      "Epoch 4/25\n",
      "3s - loss: 0.2028 - acc: 0.9502 - val_loss: 0.5180 - val_acc: 0.7453\n",
      "Epoch 5/25\n",
      "3s - loss: 0.0963 - acc: 0.9860 - val_loss: 0.4946 - val_acc: 0.7453\n",
      "Epoch 6/25\n",
      "3s - loss: 0.0484 - acc: 0.9938 - val_loss: 0.5041 - val_acc: 0.7453\n",
      "Epoch 7/25\n",
      "3s - loss: 0.0250 - acc: 0.9984 - val_loss: 0.4938 - val_acc: 0.7702\n",
      "Epoch 8/25\n",
      "3s - loss: 0.0191 - acc: 0.9984 - val_loss: 0.5199 - val_acc: 0.7702\n",
      "Epoch 9/25\n",
      "3s - loss: 0.0124 - acc: 1.0000 - val_loss: 0.5090 - val_acc: 0.7826\n",
      "Epoch 10/25\n",
      "3s - loss: 0.0073 - acc: 0.9984 - val_loss: 0.5275 - val_acc: 0.7578\n",
      "Epoch 11/25\n",
      "3s - loss: 0.0047 - acc: 1.0000 - val_loss: 0.5255 - val_acc: 0.7702\n",
      "Epoch 12/25\n",
      "3s - loss: 0.0037 - acc: 1.0000 - val_loss: 0.5567 - val_acc: 0.7578\n",
      "Epoch 13/25\n",
      "3s - loss: 0.0032 - acc: 1.0000 - val_loss: 0.5419 - val_acc: 0.7640\n",
      "Epoch 14/25\n",
      "3s - loss: 0.0026 - acc: 1.0000 - val_loss: 0.5443 - val_acc: 0.7640\n",
      "Epoch 15/25\n",
      "3s - loss: 0.0024 - acc: 1.0000 - val_loss: 0.5453 - val_acc: 0.7640\n",
      "Epoch 16/25\n",
      "3s - loss: 0.0018 - acc: 1.0000 - val_loss: 0.5504 - val_acc: 0.7578\n",
      "Epoch 17/25\n",
      "3s - loss: 0.0016 - acc: 1.0000 - val_loss: 0.5550 - val_acc: 0.7578\n",
      "Epoch 18/25\n",
      "3s - loss: 0.0014 - acc: 1.0000 - val_loss: 0.5591 - val_acc: 0.7578\n",
      "Epoch 19/25\n",
      "3s - loss: 0.0012 - acc: 1.0000 - val_loss: 0.5663 - val_acc: 0.7516\n",
      "Epoch 20/25\n",
      "3s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.5609 - val_acc: 0.7702\n",
      "Epoch 21/25\n",
      "3s - loss: 9.8579e-04 - acc: 1.0000 - val_loss: 0.5733 - val_acc: 0.7516\n",
      "Epoch 22/25\n",
      "3s - loss: 8.7664e-04 - acc: 1.0000 - val_loss: 0.5722 - val_acc: 0.7640\n",
      "Epoch 23/25\n",
      "3s - loss: 8.0717e-04 - acc: 1.0000 - val_loss: 0.5806 - val_acc: 0.7516\n",
      "Epoch 24/25\n",
      "3s - loss: 7.3026e-04 - acc: 1.0000 - val_loss: 0.5795 - val_acc: 0.7640\n",
      "Epoch 25/25\n",
      "3s - loss: 6.6892e-04 - acc: 1.0000 - val_loss: 0.5820 - val_acc: 0.7640\n",
      "Accuracy: 76.40%\n",
      "Average precision score: 0.72\n",
      "Recall score: 0.58\n",
      "Train on 643 samples, validate on 161 samples\n",
      "Epoch 1/25\n",
      "4s - loss: 0.6484 - acc: 0.6439 - val_loss: 0.6319 - val_acc: 0.6646\n",
      "Epoch 2/25\n",
      "3s - loss: 0.5525 - acc: 0.6905 - val_loss: 0.5347 - val_acc: 0.7267\n",
      "Epoch 3/25\n",
      "3s - loss: 0.3877 - acc: 0.8227 - val_loss: 0.4340 - val_acc: 0.7516\n",
      "Epoch 4/25\n",
      "2s - loss: 0.1958 - acc: 0.9285 - val_loss: 0.7070 - val_acc: 0.7453\n",
      "Epoch 5/25\n",
      "2s - loss: 0.0744 - acc: 0.9844 - val_loss: 0.9506 - val_acc: 0.7329\n",
      "Epoch 6/25\n",
      "2s - loss: 0.0339 - acc: 0.9938 - val_loss: 1.0112 - val_acc: 0.7516\n",
      "Epoch 7/25\n",
      "2s - loss: 0.0348 - acc: 0.9953 - val_loss: 1.0088 - val_acc: 0.7453\n",
      "Epoch 8/25\n",
      "2s - loss: 0.0430 - acc: 0.9922 - val_loss: 1.1039 - val_acc: 0.7329\n",
      "Epoch 9/25\n",
      "2s - loss: 0.0180 - acc: 0.9938 - val_loss: 1.7656 - val_acc: 0.7205\n",
      "Epoch 10/25\n",
      "3s - loss: 0.0069 - acc: 1.0000 - val_loss: 1.3226 - val_acc: 0.7702\n",
      "Epoch 11/25\n",
      "2s - loss: 0.0043 - acc: 1.0000 - val_loss: 1.4781 - val_acc: 0.7578\n",
      "Epoch 12/25\n",
      "3s - loss: 0.0026 - acc: 1.0000 - val_loss: 1.5554 - val_acc: 0.7702\n",
      "Epoch 13/25\n",
      "3s - loss: 0.0018 - acc: 1.0000 - val_loss: 1.5423 - val_acc: 0.7640\n",
      "Epoch 14/25\n",
      "3s - loss: 0.0014 - acc: 1.0000 - val_loss: 1.5677 - val_acc: 0.7826\n",
      "Epoch 15/25\n",
      "3s - loss: 0.0010 - acc: 1.0000 - val_loss: 1.5443 - val_acc: 0.7702\n",
      "Epoch 16/25\n",
      "3s - loss: 8.0739e-04 - acc: 1.0000 - val_loss: 1.5511 - val_acc: 0.7640\n",
      "Epoch 17/25\n",
      "3s - loss: 7.0344e-04 - acc: 1.0000 - val_loss: 1.5739 - val_acc: 0.7640\n",
      "Epoch 18/25\n",
      "3s - loss: 5.7979e-04 - acc: 1.0000 - val_loss: 1.5729 - val_acc: 0.7702\n",
      "Epoch 19/25\n",
      "3s - loss: 4.8930e-04 - acc: 1.0000 - val_loss: 1.5813 - val_acc: 0.7764\n",
      "Epoch 20/25\n",
      "3s - loss: 4.3414e-04 - acc: 1.0000 - val_loss: 1.6011 - val_acc: 0.7702\n",
      "Epoch 21/25\n",
      "3s - loss: 3.6370e-04 - acc: 1.0000 - val_loss: 1.6012 - val_acc: 0.7764\n",
      "Epoch 22/25\n",
      "3s - loss: 3.2341e-04 - acc: 1.0000 - val_loss: 1.6058 - val_acc: 0.7702\n",
      "Epoch 23/25\n",
      "3s - loss: 2.8677e-04 - acc: 1.0000 - val_loss: 1.6187 - val_acc: 0.7764\n",
      "Epoch 24/25\n",
      "3s - loss: 2.5929e-04 - acc: 1.0000 - val_loss: 1.6250 - val_acc: 0.7764\n",
      "Epoch 25/25\n",
      "3s - loss: 2.3681e-04 - acc: 1.0000 - val_loss: 1.6343 - val_acc: 0.7702\n",
      "Accuracy: 77.02%\n",
      "Average precision score: 0.67\n",
      "Recall score: 0.64\n"
     ]
    }
   ],
   "source": [
    "nnAccuracy = []\n",
    "nnPR = []\n",
    "nnRecall = []\n",
    "cnnAccuracy = []\n",
    "cnnPR = []\n",
    "cnnRecall = []\n",
    "for i in range(1):\n",
    "    print(\"Training session: \" + str(i))\n",
    "    #Split data into training and test set (80%/20%)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(xPadded, Y, test_size=0.2, shuffle=True, random_state=i, stratify=Y)\n",
    "    \n",
    "    #Create model\n",
    "    model = creatNnModel()\n",
    "    cnnModel = createCnnModel()\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(xTrain, yTrain, validation_data=(xTest, yTest), epochs=25, batch_size=50, verbose=2)\n",
    "    # Final evaluation of the model\n",
    "    scores = model.evaluate(xTest, yTest, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    yPred = model.predict(xTest)\n",
    "    yPredNorm = []\n",
    "    for p in yPred:\n",
    "        if p > 0.5:\n",
    "            yPredNorm.append(1)\n",
    "        else:\n",
    "            yPredNorm.append(0)\n",
    "    average_precision = average_precision_score(yTest, yPred)\n",
    "    nn_recall_score = recall_score(yTest, yPredNorm, average='binary')\n",
    "    print('Average precision score: {0:0.2f}'.format(average_precision))\n",
    "    print('Recall score: {0:0.2f}'.format(nn_recall_score))\n",
    "    nnAccuracy.append(scores[1])\n",
    "    nnPR.append(average_precision)\n",
    "    nnRecall.append(nn_recall_score)\n",
    "    \n",
    "    # Fit the model\n",
    "    cnnModel.fit(xTrain, yTrain, validation_data=(xTest, yTest), epochs=25, batch_size=50, verbose=2)\n",
    "    # Final evaluation of the model\n",
    "    cnnscores = cnnModel.evaluate(xTest, yTest, verbose=0)\n",
    "    yPred = cnnModel.predict(xTest)\n",
    "    yPredNorm = []\n",
    "    for p in yPred:\n",
    "        if p > 0.5:\n",
    "            yPredNorm.append(1)\n",
    "        else:\n",
    "            yPredNorm.append(0)\n",
    "    cnn_average_precision = average_precision_score(yTest, yPred)\n",
    "    cnn_recall_score = recall_score(yTest, yPredNorm, average='binary')\n",
    "    print(\"Accuracy: %.2f%%\" % (cnnscores[1]*100))\n",
    "    print('Average precision score: {0:0.2f}'.format(cnn_average_precision))\n",
    "    print('Recall score: {0:0.2f}'.format(cnn_recall_score))\n",
    "    cnnAccuracy.append(cnnscores[1])\n",
    "    cnnPR.append(cnn_average_precision)\n",
    "    cnnRecall.append(cnn_recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overal NN result\n",
      "accuracy: 0.7639751552795031\n",
      "precision score: 0.7188205686066103\n",
      "recall: 0.5818181818181818\n",
      "Overal CNN result\n",
      "accuracy: 0.7701863354037267\n",
      "precision score: 0.6706246305688812\n",
      "recall: 0.6363636363636364\n"
     ]
    }
   ],
   "source": [
    "nn_mean_ac = float(sum(nnAccuracy))/float(len(nnAccuracy))\n",
    "nn_mean_pr = float(sum(nnPR))/float(len(nnPR))\n",
    "nn_mean_recall = float(sum(nnRecall))/float(len(nnRecall))\n",
    "cnn_mean_ac = float(sum(cnnAccuracy))/float(len(cnnAccuracy))\n",
    "cnn_mean_pr = float(sum(cnnPR))/float(len(cnnPR))\n",
    "cnn_mean_recall = float(sum(cnnRecall))/float(len(cnnRecall))\n",
    "print(\"Overal NN result\")\n",
    "print(\"accuracy: \" + str(nn_mean_ac))\n",
    "print(\"precision score: \" + str(nn_mean_pr))\n",
    "print(\"recall: \" + str(nn_mean_recall))\n",
    "print(\"Overal CNN result\")\n",
    "print(\"accuracy: \" + str(cnn_mean_ac))\n",
    "print(\"precision score: \" + str(cnn_mean_pr))\n",
    "print(\"recall: \" + str(cnn_mean_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_model(cnnModel, to_file='cnnModel.png')\n",
    "plot_model(model, to_file='nnModel.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#Save the model\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"saved_model/chinModel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"saved_model/chinModel.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "cnnModel_json = cnnModel.to_json()\n",
    "with open(\"saved_model/chinCnnModel.json\", \"w\") as cnnJson_file:\n",
    "    cnnJson_file.write(cnnModel_json)\n",
    "# serialize weights to HDF5\n",
    "cnnModel.save_weights(\"saved_model/chinCnnModel.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('saved_model/chinCnnModel.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"saved_model/chinCnnModel.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "#loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "#print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN model for English data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read in the csv\n",
    "engDataDf = pd.read_csv(\"data_with_features/eng_cleaned_data_f.csv\", encoding='UTF-8')\n",
    "#Only take the text and sentiment columns\n",
    "engDataDf = engDataDf[['text', 'depressed']]\n",
    "#Cleaning\n",
    "for index, row in engDataDf.iterrows():\n",
    "    #Preprocessing\n",
    "    chinText, engText = tfExtractor.splitChinEng(row['text'])\n",
    "    text = tfExtractor.engPreprocessing(engText)\n",
    "    engDataDf.set_value(index,'text',text)\n",
    "#Convert data to numpy array\n",
    "X = np.array(engDataDf['text'].tolist())\n",
    "Y = np.array(engDataDf['depressed'].tolist())\n",
    "#Convert -1 label to 0\n",
    "i = 0\n",
    "for label in Y:\n",
    "    if(label == -1):\n",
    "        Y[i] = 0\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer fitting is complete\n",
      "Number of words: 7413\n"
     ]
    }
   ],
   "source": [
    "#Original number of words: 46708\n",
    "#Set top words\n",
    "topWords = 5000\n",
    "#Tokenizing the data\n",
    "tokenizer = Tokenizer(num_words=topWords)\n",
    "tokenizer.fit_on_texts(X)\n",
    "print(\"tokenizer fitting is complete\")\n",
    "xSeq = tokenizer.texts_to_sequences(X)\n",
    "wordIndex = tokenizer.word_index\n",
    "print(\"Number of words: \" + str(len(wordIndex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save tokenizer \n",
    "pickle.dump(tokenizer, open('saved_model/engTokenizer.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review mean length: 11.1305322129\n",
      "maximum review length: 17\n",
      "Done padding\n"
     ]
    }
   ],
   "source": [
    "#Get review mean length\n",
    "lengths = [len(i) for i in xSeq]\n",
    "print(\"review mean length: \" + str(np.mean(lengths)))\n",
    "#Set maximum review length to cover at least 90% of review content\n",
    "maxReviewLength = int(np.percentile(lengths, 90))\n",
    "print(\"maximum review length: \" + str(maxReviewLength))\n",
    "\n",
    "#Set paddings for review data\n",
    "xPadded = pad_sequences(xSeq, maxlen=maxReviewLength)\n",
    "print(\"Done padding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training session: 0\n",
      "Train on 2856 samples, validate on 714 samples\n",
      "Epoch 1/20\n",
      "1s - loss: 0.4745 - acc: 0.7637 - val_loss: 0.3156 - val_acc: 0.8417\n",
      "Epoch 2/20\n",
      "1s - loss: 0.1475 - acc: 0.9454 - val_loss: 0.2810 - val_acc: 0.8683\n",
      "Epoch 3/20\n",
      "1s - loss: 0.0288 - acc: 0.9923 - val_loss: 0.2939 - val_acc: 0.8697\n",
      "Epoch 4/20\n",
      "1s - loss: 0.0126 - acc: 0.9972 - val_loss: 0.3209 - val_acc: 0.8669\n",
      "Epoch 5/20\n",
      "1s - loss: 0.0081 - acc: 0.9993 - val_loss: 0.3392 - val_acc: 0.8683\n",
      "Epoch 6/20\n",
      "1s - loss: 0.0060 - acc: 0.9989 - val_loss: 0.3566 - val_acc: 0.8697\n",
      "Epoch 7/20\n",
      "1s - loss: 0.0057 - acc: 0.9989 - val_loss: 0.3718 - val_acc: 0.8697\n",
      "Epoch 8/20\n",
      "1s - loss: 0.0044 - acc: 0.9982 - val_loss: 0.4051 - val_acc: 0.8655\n",
      "Epoch 9/20\n",
      "1s - loss: 0.0050 - acc: 0.9989 - val_loss: 0.4041 - val_acc: 0.8655\n",
      "Epoch 10/20\n",
      "1s - loss: 0.0041 - acc: 0.9993 - val_loss: 0.4093 - val_acc: 0.8641\n",
      "Epoch 11/20\n",
      "1s - loss: 0.0034 - acc: 0.9989 - val_loss: 0.4676 - val_acc: 0.8543\n",
      "Epoch 12/20\n",
      "1s - loss: 0.0060 - acc: 0.9989 - val_loss: 0.4199 - val_acc: 0.8627\n",
      "Epoch 13/20\n",
      "1s - loss: 0.0041 - acc: 0.9993 - val_loss: 0.4209 - val_acc: 0.8683\n",
      "Epoch 14/20\n",
      "1s - loss: 0.0053 - acc: 0.9989 - val_loss: 0.4327 - val_acc: 0.8669\n",
      "Epoch 15/20\n",
      "1s - loss: 0.0037 - acc: 0.9986 - val_loss: 0.4492 - val_acc: 0.8669\n",
      "Epoch 16/20\n",
      "1s - loss: 0.0038 - acc: 0.9993 - val_loss: 0.4368 - val_acc: 0.8655\n",
      "Epoch 17/20\n",
      "1s - loss: 0.0040 - acc: 0.9993 - val_loss: 0.4766 - val_acc: 0.8557\n",
      "Epoch 18/20\n",
      "1s - loss: 0.0049 - acc: 0.9986 - val_loss: 0.4407 - val_acc: 0.8613\n",
      "Epoch 19/20\n",
      "1s - loss: 0.0058 - acc: 0.9982 - val_loss: 0.5005 - val_acc: 0.8417\n",
      "Epoch 20/20\n",
      "1s - loss: 0.0060 - acc: 0.9993 - val_loss: 0.4726 - val_acc: 0.8529\n",
      "Accuracy: 85.29%\n",
      "Average precision-recall score: 0.95\n",
      "Train on 2856 samples, validate on 714 samples\n",
      "Epoch 1/20\n",
      "1s - loss: 0.4707 - acc: 0.7665 - val_loss: 0.2916 - val_acc: 0.8655\n",
      "Epoch 2/20\n",
      "1s - loss: 0.1296 - acc: 0.9555 - val_loss: 0.3308 - val_acc: 0.8641\n",
      "Epoch 3/20\n",
      "1s - loss: 0.0339 - acc: 0.9891 - val_loss: 0.3912 - val_acc: 0.8655\n",
      "Epoch 4/20\n",
      "1s - loss: 0.0117 - acc: 0.9965 - val_loss: 0.4206 - val_acc: 0.8768\n",
      "Epoch 5/20\n",
      "1s - loss: 0.0085 - acc: 0.9989 - val_loss: 0.4454 - val_acc: 0.8796\n",
      "Epoch 6/20\n",
      "1s - loss: 0.0051 - acc: 0.9989 - val_loss: 0.4612 - val_acc: 0.8824\n",
      "Epoch 7/20\n",
      "1s - loss: 0.0041 - acc: 0.9993 - val_loss: 0.4879 - val_acc: 0.8739\n",
      "Epoch 8/20\n",
      "1s - loss: 0.0034 - acc: 0.9993 - val_loss: 0.5072 - val_acc: 0.8782\n",
      "Epoch 9/20\n",
      "1s - loss: 0.0041 - acc: 0.9993 - val_loss: 0.5158 - val_acc: 0.8768\n",
      "Epoch 10/20\n",
      "1s - loss: 0.0048 - acc: 0.9989 - val_loss: 0.5409 - val_acc: 0.8796\n",
      "Epoch 11/20\n",
      "1s - loss: 0.0029 - acc: 0.9993 - val_loss: 0.5499 - val_acc: 0.8782\n",
      "Epoch 12/20\n",
      "1s - loss: 0.0045 - acc: 0.9993 - val_loss: 0.5733 - val_acc: 0.8782\n",
      "Epoch 13/20\n",
      "1s - loss: 0.0034 - acc: 0.9989 - val_loss: 0.5515 - val_acc: 0.8810\n",
      "Epoch 14/20\n",
      "1s - loss: 0.0029 - acc: 0.9993 - val_loss: 0.5651 - val_acc: 0.8768\n",
      "Epoch 15/20\n",
      "1s - loss: 0.0030 - acc: 0.9993 - val_loss: 0.5623 - val_acc: 0.8810\n",
      "Epoch 16/20\n",
      "1s - loss: 0.0035 - acc: 0.9989 - val_loss: 0.5821 - val_acc: 0.8782\n",
      "Epoch 17/20\n",
      "1s - loss: 0.0027 - acc: 0.9993 - val_loss: 0.5943 - val_acc: 0.8782\n",
      "Epoch 18/20\n",
      "1s - loss: 0.0031 - acc: 0.9989 - val_loss: 0.5901 - val_acc: 0.8782\n",
      "Epoch 19/20\n",
      "1s - loss: 0.0032 - acc: 0.9989 - val_loss: 0.5790 - val_acc: 0.8838\n",
      "Epoch 20/20\n",
      "1s - loss: 0.0026 - acc: 0.9993 - val_loss: 0.6108 - val_acc: 0.8782\n",
      "Accuracy: 87.82%\n",
      "Average precision score: 0.95\n",
      "Recall score: 0.87\n"
     ]
    }
   ],
   "source": [
    "nnAccuracy = []\n",
    "nnPR = []\n",
    "nnRecall = []\n",
    "cnnAccuracy = []\n",
    "cnnPR = []\n",
    "cnnRecall = []\n",
    "for i in range(1):\n",
    "    print(\"Training session: \" + str(i))\n",
    "    #Split data into training and test set (80%/20%)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(xPadded, Y, test_size=0.2, shuffle=True, random_state=i, stratify=Y)\n",
    "    \n",
    "    #Create model\n",
    "    engModel = creatNnModel()\n",
    "    engCnnModel = createCnnModel()\n",
    "    \n",
    "    # Fit the model\n",
    "    engModel.fit(xTrain, yTrain, validation_data=(xTest, yTest), epochs=20, batch_size=50, verbose=2)\n",
    "    # Final evaluation of the model\n",
    "    scores = engModel.evaluate(xTest, yTest, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    yPred = engModel.predict(xTest)\n",
    "    yPredNorm = []\n",
    "    for p in yPred:\n",
    "        if p > 0.5:\n",
    "            yPredNorm.append(1)\n",
    "        else:\n",
    "            yPredNorm.append(0)\n",
    "    average_precision = average_precision_score(yTest, yPred)\n",
    "    nn_recall_score = recall_score(yTest, yPredNorm, average='binary')\n",
    "    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "    nnAccuracy.append(scores[1])\n",
    "    nnPR.append(average_precision)\n",
    "    nnRecall.append(nn_recall_score)\n",
    "    \n",
    "    # Fit the model\n",
    "    engCnnModel.fit(xTrain, yTrain, validation_data=(xTest, yTest), epochs=20, batch_size=50, verbose=2)\n",
    "    # Final evaluation of the model\n",
    "    cnnscores = engCnnModel.evaluate(xTest, yTest, verbose=0)\n",
    "    yPred = engCnnModel.predict(xTest)\n",
    "    yPredNorm = []\n",
    "    for p in yPred:\n",
    "        if p > 0.5:\n",
    "            yPredNorm.append(1)\n",
    "        else:\n",
    "            yPredNorm.append(0)\n",
    "    cnn_average_precision = average_precision_score(yTest, yPred)\n",
    "    cnn_recall_score = recall_score(yTest, yPredNorm, average='binary')\n",
    "    print(\"Accuracy: %.2f%%\" % (cnnscores[1]*100))\n",
    "    print('Average precision score: {0:0.2f}'.format(cnn_average_precision))\n",
    "    print('Recall score: {0:0.2f}'.format(cnn_recall_score))\n",
    "    cnnAccuracy.append(cnnscores[1])\n",
    "    cnnPR.append(cnn_average_precision)\n",
    "    cnnRecall.append(cnn_recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_mean_ac = float(sum(nnAccuracy))/float(len(nnAccuracy))\n",
    "nn_mean_pr = float(sum(nnPR))/float(len(nnPR))\n",
    "nn_mean_recall = float(sum(nnRecall))/float(len(nnRecall))\n",
    "cnn_mean_ac = float(sum(cnnAccuracy))/float(len(cnnAccuracy))\n",
    "cnn_mean_pr = float(sum(cnnPR))/float(len(cnnPR))\n",
    "cnn_mean_recall = float(sum(cnnRecall))/float(len(cnnRecall))\n",
    "print(\"Overal NN result\")\n",
    "print(\"accuracy: \" + str(nn_mean_ac))\n",
    "print(\"precision score: \" + str(nn_mean_pr))\n",
    "print(\"recall: \" + str(nn_mean_recall))\n",
    "print(\"Overal CNN result\")\n",
    "print(\"accuracy: \" + str(cnn_mean_ac))\n",
    "print(\"precision score: \" + str(cnn_mean_pr))\n",
    "print(\"recall: \" + str(cnn_mean_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the model\n",
    "# serialize model to JSON\n",
    "model_json = engModel.to_json()\n",
    "with open(\"saved_model/engModel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "engModel.save_weights(\"saved_model/engModel.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "cnnModel_json = engCnnModel.to_json()\n",
    "with open(\"saved_model/engCnnModel.json\", \"w\") as cnnJson_file:\n",
    "    cnnJson_file.write(cnnModel_json)\n",
    "# serialize weights to HDF5\n",
    "engCnnModel.save_weights(\"saved_model/engCnnModel.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yPred = cnnModel.predict(xTest)\n",
    "average_precision = average_precision_score(yTest, yPred)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(average_precision))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
